{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1: scikit-learn\n",
    "## TA: Suraj Yerramilli\n",
    "## Date: January 18th, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is a popular machine learning library that has implementations of a large number of common algorithms, including ones for text processing. An advantage of scikit-learn is a consistent syntax across different kinds of models.\n",
    "\n",
    "Before starting, ensure you have the sklearn library (version >=0.20) installed. You can either use the Anaconda Navigator or the command line for installing/upgrading.\n",
    "\n",
    "**pip**:\n",
    "\n",
    "```{bash}\n",
    "pip install --user sklearn\n",
    "pip install --user --upgrade sklearn\n",
    "```\n",
    "\n",
    "**conda**:\n",
    "\n",
    "```{bash}\n",
    "conda install sklearn\n",
    "conda update sklearn\n",
    "```\n",
    "\n",
    "We will start with a script which performs Ridge regression on the california housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Typical sklearn script\n",
    "\n",
    "# importing libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# read the housing data and extract features and labels\n",
    "housing = np.genfromtxt(\"../data/california_housing.csv\",delimiter=\",\",names=None,skip_header=1)\n",
    "X = housing[:,:-1]\n",
    "y = housing[:,-1]\n",
    "\n",
    "# Split into train and test sets (2:1 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# preprocessing the data\n",
    "scaler = StandardScaler()\n",
    "X_sc = scaler.fit_transform(X_train)\n",
    "\n",
    "# initialize regressor and fit object\n",
    "reg = Ridge(alpha=1e-2) # alpha is the regularization parameter\n",
    "reg.fit(X_sc,y_train)\n",
    "\n",
    "# Obtain predictions\n",
    "y_pred_train = reg.predict(X_sc) \n",
    "X_sc_test = scaler.transform(X_test)  # need to normalize (with the same mean and sd) before obtaining predictions\n",
    "y_pred_test = reg.predict(X_sc_test)\n",
    "\n",
    "\n",
    "# Report training and test performance\n",
    "print(\"Training MSE: {}\".format(mean_squared_error(y_train,y_pred_train)))\n",
    "print(\"Training R^2: {}\".format(r2_score(y_train,y_pred_train)))\n",
    "print()\n",
    "print(\"Test MSE: {}\".format(mean_squared_error(y_test,y_pred_test)))\n",
    "print(\"Test R^2: {}\".format(r2_score(y_test,y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg.coef_ # regression coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the individual components of the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "In most cases, the data input will be in the form of numpy arrays. There may be other data types for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shapes of the different arrays\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Sets (Model Validation)\n",
    "\n",
    "Training and testing the model on the same data is a mistake. You can obtain near-perfect fits to the data while working poorly on unseen data. So, the typical strategy is to hold-out some part of the data as a test set. The `train_test_split` function does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_test_split` function is part of the `sklearn.model_selection` module. It has other validaiton strategies like KFold (which is more robust)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main API implemented by scikit-learn is the Estimator class. An estimator is any object that learns from data; it may be a classification, regression or clustering algorithm or a transformer that extracts/filters useful features from raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of transforming estimators -StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder. These are availabel in the `sklearn.preprocessing` module. \n",
    "\n",
    "Estimator classes for transformers typically have the following methods:\n",
    "\n",
    "| Method                | Description                                                  |\n",
    "|-----------------------|--------------------------------------------------------------|\n",
    "| .fit(X)               | Compute transformer \"parameters\" from data                   |\n",
    "| .transform(X)         | Transform new data                                           |\n",
    "| .fit_transform(X)     | Compute transformer \"parameters\" and return transformed data |\n",
    "| .inverse_transform(X) | Return the original representation on new data               |\n",
    "\n",
    "In the code, we use the StandardScaler to normalize the features in the training set to have zero mean and standard deviation. Note that the test data should be scaled with the same mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fitted parameters of scaler object\n",
    "{'mean':scaler.mean_,'sd': scaler.scale_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator classes for supervised models typically have the following methods:\n",
    "\n",
    "| Method             | Description                                                         |\n",
    "|--------------------|---------------------------------------------------------------------|\n",
    "| .fit(X,y)          | Fits the model on the data. y is not needed for unsupervised models |\n",
    "| .predict(X)        | Returns predictions (either values or labels) on the new data       |\n",
    "| .predict_proba(X)  | Returns class probabilities on the new data (when applicable)       |\n",
    "| .get_params()      | Get the model's tuning parameters (defined in object call)          |\n",
    "| .score(X,y)        | Returns the prediction score on the new data                        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sklearn.metrics` module contains functions which return some measure of prediction performance. They are of two types - loss/error and score. The typical syntax is as follows:\n",
    "\n",
    "```{python}\n",
    "loss_or_score_function(y_true,y_pred)\n",
    "```\n",
    "\n",
    "Note that the syntax is **different** for unsupervised learning metrics like clustering.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Regression - mean_squared_error, r2_score, mean_absolute_error\n",
    "Classification - accuracy_score, precision_score, recall_score, log_loss\n",
    "Clustering - silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 ##\n",
    "\n",
    "Fit a radial-basis support vector machine for regression on the california housing dataset. For classification, the estimator class is SVR. Initialize the SVR with arguments `random_state` = 1, `kernel` = 'rbf',`gamma` = 0.125 and `C` = 1. Compare its prediction performance with that of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train SVM on the california housing dataset\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "### YOUR CODE GOES HERE\n",
    "### You can use the variables defined previously\n",
    "### Note: Fitting the SVM can take upto several minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (optional)\n",
    "\n",
    "Train your favorite classifier (from sklearn of course) on the digits dataset. Return the accuracy score and log(istic) loss (when probabilities are computed). To obtain predicted probabilities use the `.predict_proba` method instead of the regular `.predict` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# load digits dataset\n",
    "X,y = load_digits(return_X_y=True)\n",
    "\n",
    "### YOUR CODE GOES HERE\n",
    "# use multiple blocks if you want to do a little more exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
